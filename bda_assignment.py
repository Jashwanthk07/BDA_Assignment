# -*- coding: utf-8 -*-
"""BDA_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c-CCE8Kx0h-dZJmf2qcK5Uwlr6HMhUmR
"""

# 1. Build a Classification Model with Spark with a dataset of your choice

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

# 1. Start Spark Session
spark = SparkSession.builder.appName("WineQualityClassification").getOrCreate()

# 2. Load dataset (replace with actual CSV path or URL)
wine_url = "/content/winequality-red.csv"  # Make sure this file is uploaded or available
data = spark.read.csv(wine_url, header=True, inferSchema=True, sep=';')

# 3. Index the target label column ('quality')
label_indexer = StringIndexer(inputCol="quality", outputCol="label")

# 4. Assemble features into a single vector
features = [
    "fixed acidity", "volatile acidity", "citric acid", "residual sugar",
    "chlorides", "free sulfur dioxide", "total sulfur dioxide", "density",
    "pH", "sulphates", "alcohol"
]
vector_assembler = VectorAssembler(inputCols=features, outputCol="features")

# 5. Create Logistic Regression model
lr_model = LogisticRegression(featuresCol="features", labelCol="label", maxIter=10)

# 6. Build pipeline
pipeline = Pipeline(stages=[label_indexer, vector_assembler, lr_model])

# 7. Train-test split
train_set, test_set = data.randomSplit([0.75, 0.25], seed=123)

# 8. Train the model
model = pipeline.fit(train_set)

# 9. Predict on test data
results = model.transform(test_set)

# 10. Evaluate model
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
acc = evaluator.evaluate(results)
print(f"Test Accuracy: {acc:.2f}")

# 11. Show predictions
results.select("features", "label", "prediction").show(5, truncate=False)

# 12. Stop Spark session
spark.stop()

# 2. Build  a Clustering Model with Spark with a dataset of your choice

# Step 1: Start Spark Session
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("WineClustering").getOrCreate()

# Step 2: Load Dataset (Assumes winequality-red.csv is in same directory or uploaded in Colab)
df = spark.read.csv("winequality-red.csv", header=True, inferSchema=True, sep=';')

# Step 3: Assemble Features
from pyspark.ml.feature import VectorAssembler

feature_columns = df.columns
feature_columns.remove('quality')  # 'quality' is the label, we'll ignore it for clustering

assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
data = assembler.transform(df).select("features")

# Step 4: Apply KMeans Clustering
from pyspark.ml.clustering import KMeans

kmeans = KMeans(featuresCol="features", k=3, seed=42)  # You can try different k values
model = kmeans.fit(data)
predictions = model.transform(data)

# Step 5: Show Cluster Assignments
predictions.show(10)

# Step 6: Evaluate with Silhouette Score
from pyspark.ml.evaluation import ClusteringEvaluator

evaluator = ClusteringEvaluator(featuresCol="features", predictionCol="prediction", metricName="silhouette")
silhouette = evaluator.evaluate(predictions)
print(f"Silhouette Score = {silhouette:.2f}")

# Step 7: Stop Spark Session
spark.stop()

#Question 3
# Step 1: Import and Start Spark
from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id, rand

spark = SparkSession.builder.appName("WineRecommendationEngine").getOrCreate()

# Step 2: Load Dataset
df_raw = spark.read.csv("winequality-red.csv", header=True, inferSchema=True, sep=';')

# Step 3: Simulate wineId and userId
df_with_ids = df_raw.withColumn("wineId", monotonically_increasing_id()) \
                    .withColumn("userId", ((rand() * 10).cast("int") + 1))  # Simulating 10 tasters

# Step 4: Create rating DataFrame
ratings = df_with_ids.select("userId", "wineId", "quality").withColumnRenamed("quality", "rating")
ratings.show(5)

# Step 5: Build ALS Model
from pyspark.ml.recommendation import ALS

als = ALS(
    userCol="userId",
    itemCol="wineId",
    ratingCol="rating",
    maxIter=10,
    regParam=0.1,
    rank=10,
    nonnegative=True,
    coldStartStrategy="drop"
)

model = als.fit(ratings)

# Step 6: Generate Top-N Recommendations
userRecs = model.recommendForAllUsers(5)
itemRecs = model.recommendForAllItems(5)

print("Top-5 wine recommendations for each user:")
userRecs.show(truncate=False)

print("Top-5 user recommendations for each wine:")
itemRecs.show(truncate=False)

# Step 7: Evaluate the Model
from pyspark.ml.evaluation import RegressionEvaluator

predictions = model.transform(ratings)
evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")
rmse = evaluator.evaluate(predictions)
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

# Optional: Show predictions
print("Sample predictions:")
predictions.select("userId", "wineId", "rating", "prediction").show(10)

# Step 8: Stop Spark
spark.stop()